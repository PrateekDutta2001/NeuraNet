<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>NeuraNet - Master Neural Networks from Beginner to Advanced</title>
    <link rel="stylesheet" href="styles.css">
    <link rel="preconnect" href="https://fonts.googleapis.com">
    <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
    <link href="https://fonts.googleapis.com/css2?family=Inter:wght@300;400;500;600;700;800&family=JetBrains+Mono:wght@400;500&display=swap" rel="stylesheet">
</head>
<body>
    <!-- Navigation -->
    <nav class="navbar" id="navbar">
        <div class="nav-container">
            <div class="logo">
                <span class="logo-icon">üß†</span>
                <span class="logo-text">NeuraNet</span>
            </div>
            <div class="nav-links">
                <a href="#home" class="nav-link active">Home</a>
                <a href="#basics" class="nav-link">Basics</a>
                <a href="#architecture" class="nav-link">Architecture</a>
                <a href="#training" class="nav-link">Training</a>
                <a href="#types" class="nav-link">Types</a>
                <a href="#advanced" class="nav-link">Advanced</a>
            </div>
            <button class="mobile-menu-btn" id="mobileMenuBtn">
                <span></span>
                <span></span>
                <span></span>
            </button>
        </div>
    </nav>

    <!-- Hero Section -->
    <section id="home" class="hero">
        <div class="hero-background">
            <div class="neural-network-bg"></div>
        </div>
        <div class="hero-content">
            <h1 class="hero-title">
                Master <span class="gradient-text">Neural Networks</span>
            </h1>
            <p class="hero-subtitle">
                A comprehensive guide from basics to advanced concepts. Learn how artificial neural networks work, understand the mathematics, and build intelligent systems.
            </p>
            <div class="hero-buttons">
                <a href="#basics" class="btn btn-primary">Start Learning</a>
                <a href="#architecture" class="btn btn-secondary">Explore Architecture</a>
            </div>
            <div class="hero-stats">
                <div class="stat">
                    <div class="stat-number">15+</div>
                    <div class="stat-label">Topics Covered</div>
                </div>
                <div class="stat">
                    <div class="stat-number">50+</div>
                    <div class="stat-label">Concepts Explained</div>
                </div>
                <div class="stat">
                    <div class="stat-number">100%</div>
                    <div class="stat-label">Interactive</div>
                </div>
            </div>
        </div>
    </section>

    <!-- Introduction Section -->
    <section id="intro" class="section">
        <div class="container">
            <div class="section-header">
                <span class="section-badge">Introduction</span>
                <h2 class="section-title">What are Neural Networks?</h2>
                <p class="section-description">
                    Neural networks are computational models inspired by the human brain, designed to recognize patterns and solve complex problems.
                </p>
            </div>
            <div class="content-grid">
                <div class="content-card">
                    <div class="card-icon">üß¨</div>
                    <h3>Biological Inspiration</h3>
                    <p>Neural networks are inspired by biological neurons in the human brain. Just as neurons communicate through electrical and chemical signals, artificial neural networks process information through interconnected nodes.</p>
                    <ul class="feature-list">
                        <li>Mimics brain's neural structure</li>
                        <li>Learning from experience</li>
                        <li>Pattern recognition capabilities</li>
                        <li>Parallel processing architecture</li>
                    </ul>
                </div>
                <div class="content-card">
                    <div class="card-icon">‚ö°</div>
                    <h3>Key Characteristics</h3>
                    <p>Neural networks possess unique characteristics that make them powerful tools for machine learning and artificial intelligence applications.</p>
                    <ul class="feature-list">
                        <li>Non-linear processing</li>
                        <li>Adaptive learning</li>
                        <li>Fault tolerance</li>
                        <li>Generalization ability</li>
                    </ul>
                </div>
                <div class="content-card">
                    <div class="card-icon">üéØ</div>
                    <h3>Applications</h3>
                    <p>Neural networks are used in various real-world applications, revolutionizing industries from healthcare to autonomous systems.</p>
                    <ul class="feature-list">
                        <li>Image & speech recognition</li>
                        <li>Natural language processing</li>
                        <li>Medical diagnosis</li>
                        <li>Autonomous vehicles</li>
                    </ul>
                </div>
            </div>
        </div>
    </section>

    <!-- Basics Section -->
    <section id="basics" class="section section-dark">
        <div class="container">
            <div class="section-header">
                <span class="section-badge">Fundamentals</span>
                <h2 class="section-title">Neural Network Basics</h2>
                <p class="section-description">
                    Understanding the fundamental building blocks of neural networks
                </p>
            </div>

            <!-- Perceptron -->
            <div class="topic-block">
                <h3 class="topic-title">
                    <span class="topic-number">1</span>
                    The Perceptron - Building Block
                </h3>
                <div class="topic-content">
                    <div class="text-content">
                        <p>The perceptron is the simplest form of a neural network, consisting of a single neuron. It was invented by Frank Rosenblatt in 1958 and laid the foundation for modern neural networks.</p>
                        <div class="formula-box">
                            <div class="formula-title">Perceptron Function</div>
                            <code>y = f(w‚ÇÅx‚ÇÅ + w‚ÇÇx‚ÇÇ + ... + w‚Çôx‚Çô + b)</code>
                            <p class="formula-explanation">
                                where: x = inputs, w = weights, b = bias, f = activation function
                            </p>
                        </div>
                        <h4>Components:</h4>
                        <ul class="detailed-list">
                            <li><strong>Inputs (x):</strong> Features or attributes fed into the neuron</li>
                            <li><strong>Weights (w):</strong> Parameters that determine the importance of each input</li>
                            <li><strong>Bias (b):</strong> Offset value that helps the model fit the data better</li>
                            <li><strong>Activation Function (f):</strong> Introduces non-linearity to the model</li>
                            <li><strong>Output (y):</strong> Final prediction or classification</li>
                        </ul>
                    </div>
                    <div class="visual-content">
                        <div class="diagram perceptron-diagram">
                            <svg viewBox="0 0 400 300" class="interactive-svg">
                                <!-- Inputs -->
                                <circle cx="50" cy="50" r="15" fill="#667eea" />
                                <text x="25" y="55" fill="#fff" font-size="12">x‚ÇÅ</text>
                                <circle cx="50" cy="150" r="15" fill="#667eea" />
                                <text x="25" y="155" fill="#fff" font-size="12">x‚ÇÇ</text>
                                <circle cx="50" cy="250" r="15" fill="#667eea" />
                                <text x="25" y="255" fill="#fff" font-size="12">x‚ÇÉ</text>
                                
                                <!-- Neuron -->
                                <circle cx="200" cy="150" r="40" fill="#764ba2" class="pulse" />
                                <text x="190" y="155" fill="#fff" font-size="14">Œ£</text>
                                
                                <!-- Lines -->
                                <line x1="65" y1="50" x2="160" y2="140" stroke="#667eea" stroke-width="2" />
                                <text x="100" y="90" fill="#667eea" font-size="12">w‚ÇÅ</text>
                                <line x1="65" y1="150" x2="160" y2="150" stroke="#667eea" stroke-width="2" />
                                <text x="100" y="145" fill="#667eea" font-size="12">w‚ÇÇ</text>
                                <line x1="65" y1="250" x2="160" y2="160" stroke="#667eea" stroke-width="2" />
                                <text x="100" y="215" fill="#667eea" font-size="12">w‚ÇÉ</text>
                                
                                <!-- Bias -->
                                <circle cx="200" cy="50" r="15" fill="#f093fb" />
                                <text x="190" y="55" fill="#fff" font-size="12">b</text>
                                <line x1="200" y1="65" x2="200" y2="110" stroke="#f093fb" stroke-width="2" />
                                
                                <!-- Activation -->
                                <rect x="260" y="130" width="60" height="40" rx="5" fill="#f093fb" />
                                <text x="275" y="155" fill="#fff" font-size="12">f(x)</text>
                                <line x1="240" y1="150" x2="260" y2="150" stroke="#764ba2" stroke-width="2" />
                                
                                <!-- Output -->
                                <circle cx="360" cy="150" r="15" fill="#4facfe" />
                                <text x="350" y="155" fill="#fff" font-size="12">y</text>
                                <line x1="320" y1="150" x2="345" y2="150" stroke="#f093fb" stroke-width="2" />
                            </svg>
                        </div>
                    </div>
                </div>
            </div>

            <!-- Activation Functions -->
            <div class="topic-block">
                <h3 class="topic-title">
                    <span class="topic-number">2</span>
                    Activation Functions
                </h3>
                <div class="topic-content full-width">
                    <p>Activation functions introduce non-linearity into the network, enabling it to learn complex patterns. Without activation functions, neural networks would only be able to learn linear relationships.</p>
                    <div class="activation-grid">
                        <div class="activation-card">
                            <h4>Sigmoid</h4>
                            <div class="formula-box">
                                <code>œÉ(x) = 1 / (1 + e‚ÅªÀ£)</code>
                            </div>
                            <p><strong>Range:</strong> (0, 1)</p>
                            <p><strong>Use Case:</strong> Binary classification, output layer</p>
                            <p><strong>Pros:</strong> Smooth gradient, clear predictions</p>
                            <p><strong>Cons:</strong> Vanishing gradient problem, not zero-centered</p>
                        </div>
                        <div class="activation-card">
                            <h4>Tanh (Hyperbolic Tangent)</h4>
                            <div class="formula-box">
                                <code>tanh(x) = (eÀ£ - e‚ÅªÀ£) / (eÀ£ + e‚ÅªÀ£)</code>
                            </div>
                            <p><strong>Range:</strong> (-1, 1)</p>
                            <p><strong>Use Case:</strong> Hidden layers in RNNs</p>
                            <p><strong>Pros:</strong> Zero-centered, stronger gradients than sigmoid</p>
                            <p><strong>Cons:</strong> Still suffers from vanishing gradient</p>
                        </div>
                        <div class="activation-card">
                            <h4>ReLU (Rectified Linear Unit)</h4>
                            <div class="formula-box">
                                <code>ReLU(x) = max(0, x)</code>
                            </div>
                            <p><strong>Range:</strong> [0, ‚àû)</p>
                            <p><strong>Use Case:</strong> Most popular for hidden layers</p>
                            <p><strong>Pros:</strong> Computationally efficient, no vanishing gradient</p>
                            <p><strong>Cons:</strong> Dead neurons problem</p>
                        </div>
                        <div class="activation-card">
                            <h4>Leaky ReLU</h4>
                            <div class="formula-box">
                                <code>f(x) = max(Œ±x, x)</code>
                            </div>
                            <p><strong>Range:</strong> (-‚àû, ‚àû)</p>
                            <p><strong>Use Case:</strong> Alternative to ReLU</p>
                            <p><strong>Pros:</strong> Prevents dead neurons, allows small negative values</p>
                            <p><strong>Cons:</strong> Inconsistent predictions for negative values</p>
                        </div>
                        <div class="activation-card">
                            <h4>Softmax</h4>
                            <div class="formula-box">
                                <code>œÉ(x)·µ¢ = eÀ£‚Å± / Œ£‚±º eÀ£ ≤</code>
                            </div>
                            <p><strong>Range:</strong> (0, 1), sum = 1</p>
                            <p><strong>Use Case:</strong> Multi-class classification output</p>
                            <p><strong>Pros:</strong> Probability distribution, interpretable</p>
                            <p><strong>Cons:</strong> Computational overhead</p>
                        </div>
                        <div class="activation-card">
                            <h4>ELU (Exponential Linear Unit)</h4>
                            <div class="formula-box">
                                <code>f(x) = x if x > 0, Œ±(eÀ£ - 1) if x ‚â§ 0</code>
                            </div>
                            <p><strong>Range:</strong> (-Œ±, ‚àû)</p>
                            <p><strong>Use Case:</strong> Deep networks</p>
                            <p><strong>Pros:</strong> Smooth, zero-centered, reduces bias shift</p>
                            <p><strong>Cons:</strong> More computation than ReLU</p>
                        </div>
                    </div>
                </div>
            </div>

            <!-- Weights and Biases -->
            <div class="topic-block">
                <h3 class="topic-title">
                    <span class="topic-number">3</span>
                    Weights and Biases
                </h3>
                <div class="topic-content">
                    <div class="text-content">
                        <h4>Weights (w)</h4>
                        <p>Weights are the learnable parameters that determine the strength of connections between neurons. They control how much influence each input has on the output.</p>
                        <ul class="detailed-list">
                            <li><strong>Initialization:</strong> Proper weight initialization is crucial (Xavier, He, Random)</li>
                            <li><strong>Learning:</strong> Weights are adjusted during training through backpropagation</li>
                            <li><strong>Impact:</strong> Larger weights mean stronger influence on the output</li>
                            <li><strong>Regularization:</strong> Techniques like L1/L2 prevent weights from becoming too large</li>
                        </ul>
                        
                        <h4>Biases (b)</h4>
                        <p>Bias is an additional parameter that allows the activation function to be shifted left or right, helping the model fit the data better.</p>
                        <ul class="detailed-list">
                            <li><strong>Purpose:</strong> Provides flexibility in fitting the data</li>
                            <li><strong>Independence:</strong> Unlike weights, bias is not multiplied by input</li>
                            <li><strong>Initialization:</strong> Usually initialized to zero or small values</li>
                            <li><strong>Role:</strong> Helps the model make predictions even when all inputs are zero</li>
                        </ul>
                    </div>
                    <div class="visual-content">
                        <div class="info-box">
                            <h4>Weight Matrix Representation</h4>
                            <p>In matrix form, the operation of a layer can be represented as:</p>
                            <div class="formula-box">
                                <code>Y = f(W¬∑X + B)</code>
                            </div>
                            <p>Where:</p>
                            <ul>
                                <li>W = weight matrix</li>
                                <li>X = input vector</li>
                                <li>B = bias vector</li>
                                <li>f = activation function</li>
                                <li>Y = output vector</li>
                            </ul>
                        </div>
                    </div>
                </div>
            </div>
        </div>
    </section>

    <!-- Architecture Section -->
    <section id="architecture" class="section">
        <div class="container">
            <div class="section-header">
                <span class="section-badge">Structure</span>
                <h2 class="section-title">Neural Network Architecture</h2>
                <p class="section-description">
                    Exploring the layers and structure of neural networks
                </p>
            </div>

            <!-- Layers -->
            <div class="topic-block">
                <h3 class="topic-title">
                    <span class="topic-number">4</span>
                    Network Layers
                </h3>
                <div class="architecture-layers">
                    <div class="layer-card">
                        <div class="layer-icon input-layer">IN</div>
                        <h4>Input Layer</h4>
                        <p>The first layer that receives raw data. Each neuron represents one feature of the input data.</p>
                        <ul class="feature-list">
                            <li>Number of neurons = number of features</li>
                            <li>No computation, just passes data forward</li>
                            <li>Example: 784 neurons for 28x28 pixel image</li>
                        </ul>
                    </div>
                    <div class="layer-card">
                        <div class="layer-icon hidden-layer">H</div>
                        <h4>Hidden Layers</h4>
                        <p>Intermediate layers that perform computations and extract features. The "deep" in deep learning refers to multiple hidden layers.</p>
                        <ul class="feature-list">
                            <li>Can have multiple hidden layers</li>
                            <li>Each layer learns different level of abstraction</li>
                            <li>First layers: simple features (edges, colors)</li>
                            <li>Deeper layers: complex features (faces, objects)</li>
                        </ul>
                    </div>
                    <div class="layer-card">
                        <div class="layer-icon output-layer">OUT</div>
                        <h4>Output Layer</h4>
                        <p>The final layer that produces predictions. Number of neurons depends on the task.</p>
                        <ul class="feature-list">
                            <li>Binary classification: 1 neuron (sigmoid)</li>
                            <li>Multi-class: n neurons (softmax)</li>
                            <li>Regression: 1 or more neurons (linear)</li>
                        </ul>
                    </div>
                </div>
            </div>

            <!-- Forward Propagation -->
            <div class="topic-block">
                <h3 class="topic-title">
                    <span class="topic-number">5</span>
                    Forward Propagation
                </h3>
                <div class="topic-content">
                    <div class="text-content">
                        <p>Forward propagation is the process of passing input data through the network to generate an output. It's called "forward" because data flows from input to output.</p>
                        
                        <h4>Step-by-Step Process:</h4>
                        <ol class="step-list">
                            <li>
                                <strong>Input Layer:</strong> Receive input data
                                <div class="formula-box small">
                                    <code>a‚ÅΩ‚Å∞‚Åæ = X</code>
                                </div>
                            </li>
                            <li>
                                <strong>Hidden Layer Computation:</strong> For each layer l
                                <div class="formula-box small">
                                    <code>z‚ÅΩÀ°‚Åæ = W‚ÅΩÀ°‚Åæ¬∑a‚ÅΩÀ°‚Åª¬π‚Åæ + b‚ÅΩÀ°‚Åæ</code>
                                    <code>a‚ÅΩÀ°‚Åæ = f(z‚ÅΩÀ°‚Åæ)</code>
                                </div>
                            </li>
                            <li>
                                <strong>Output Layer:</strong> Final prediction
                                <div class="formula-box small">
                                    <code>≈∑ = a‚ÅΩ·¥∏‚Åæ</code>
                                </div>
                            </li>
                            <li>
                                <strong>Loss Calculation:</strong> Compare prediction with actual
                                <div class="formula-box small">
                                    <code>Loss = L(≈∑, y)</code>
                                </div>
                            </li>
                        </ol>
                        
                        <div class="info-box">
                            <h4>Key Terms:</h4>
                            <ul>
                                <li><strong>z:</strong> Pre-activation (weighted sum + bias)</li>
                                <li><strong>a:</strong> Activation (after applying activation function)</li>
                                <li><strong>W:</strong> Weight matrix</li>
                                <li><strong>b:</strong> Bias vector</li>
                                <li><strong>l:</strong> Layer number</li>
                            </ul>
                        </div>
                    </div>
                    <div class="visual-content">
                        <div class="diagram">
                            <div class="forward-prop-visual">
                                <div class="prop-step">
                                    <div class="step-label">Input</div>
                                    <div class="neurons">
                                        <div class="neuron">x‚ÇÅ</div>
                                        <div class="neuron">x‚ÇÇ</div>
                                        <div class="neuron">x‚ÇÉ</div>
                                    </div>
                                </div>
                                <div class="arrow">‚Üí</div>
                                <div class="prop-step">
                                    <div class="step-label">Hidden 1</div>
                                    <div class="neurons">
                                        <div class="neuron">h‚ÇÅ</div>
                                        <div class="neuron">h‚ÇÇ</div>
                                        <div class="neuron">h‚ÇÉ</div>
                                        <div class="neuron">h‚ÇÑ</div>
                                    </div>
                                </div>
                                <div class="arrow">‚Üí</div>
                                <div class="prop-step">
                                    <div class="step-label">Hidden 2</div>
                                    <div class="neurons">
                                        <div class="neuron">h‚ÇÅ</div>
                                        <div class="neuron">h‚ÇÇ</div>
                                        <div class="neuron">h‚ÇÉ</div>
                                    </div>
                                </div>
                                <div class="arrow">‚Üí</div>
                                <div class="prop-step">
                                    <div class="step-label">Output</div>
                                    <div class="neurons">
                                        <div class="neuron">≈∑</div>
                                    </div>
                                </div>
                            </div>
                        </div>
                    </div>
                </div>
            </div>

            <!-- Network Depth and Width -->
            <div class="topic-block">
                <h3 class="topic-title">
                    <span class="topic-number">6</span>
                    Network Depth and Width
                </h3>
                <div class="topic-content full-width">
                    <div class="comparison-grid">
                        <div class="comparison-card">
                            <h4>Shallow Networks</h4>
                            <p>Networks with few hidden layers (1-2 layers)</p>
                            <ul class="pros-cons">
                                <li class="pro">‚úì Faster to train</li>
                                <li class="pro">‚úì Less prone to overfitting</li>
                                <li class="pro">‚úì Easier to debug</li>
                                <li class="con">‚úó Limited learning capacity</li>
                                <li class="con">‚úó Can't learn complex patterns</li>
                            </ul>
                            <p><strong>Best for:</strong> Simple problems, small datasets</p>
                        </div>
                        <div class="comparison-card">
                            <h4>Deep Networks</h4>
                            <p>Networks with many hidden layers (3+ layers)</p>
                            <ul class="pros-cons">
                                <li class="pro">‚úì Learn hierarchical features</li>
                                <li class="pro">‚úì Better for complex tasks</li>
                                <li class="pro">‚úì State-of-the-art performance</li>
                                <li class="con">‚úó Requires more data</li>
                                <li class="con">‚úó Longer training time</li>
                                <li class="con">‚úó Risk of overfitting</li>
                            </ul>
                            <p><strong>Best for:</strong> Complex problems, large datasets</p>
                        </div>
                        <div class="comparison-card">
                            <h4>Wide Networks</h4>
                            <p>Networks with many neurons per layer</p>
                            <ul class="pros-cons">
                                <li class="pro">‚úì More parameters to learn</li>
                                <li class="pro">‚úì Better feature representation</li>
                                <li class="con">‚úó More memory required</li>
                                <li class="con">‚úó Slower computation</li>
                                <li class="con">‚úó Risk of overfitting</li>
                            </ul>
                            <p><strong>Best for:</strong> High-dimensional data</p>
                        </div>
                    </div>
                </div>
            </div>
        </div>
    </section>

    <!-- Training Section -->
    <section id="training" class="section section-dark">
        <div class="container">
            <div class="section-header">
                <span class="section-badge">Learning</span>
                <h2 class="section-title">Training Neural Networks</h2>
                <p class="section-description">
                    Understanding how neural networks learn from data
                </p>
            </div>

            <!-- Backpropagation -->
            <div class="topic-block">
                <h3 class="topic-title">
                    <span class="topic-number">7</span>
                    Backpropagation Algorithm
                </h3>
                <div class="topic-content">
                    <div class="text-content">
                        <p>Backpropagation is the cornerstone of neural network training. It efficiently computes gradients of the loss function with respect to all weights in the network using the chain rule of calculus.</p>
                        
                        <h4>How It Works:</h4>
                        <ol class="step-list">
                            <li>
                                <strong>Forward Pass:</strong> Compute predictions and loss
                            </li>
                            <li>
                                <strong>Compute Output Gradient:</strong>
                                <div class="formula-box small">
                                    <code>‚àÇL/‚àÇa‚ÅΩ·¥∏‚Åæ = ‚àÇL/‚àÇ≈∑</code>
                                </div>
                            </li>
                            <li>
                                <strong>Backward Pass:</strong> For each layer from L to 1
                                <div class="formula-box small">
                                    <code>‚àÇL/‚àÇz‚ÅΩÀ°‚Åæ = ‚àÇL/‚àÇa‚ÅΩÀ°‚Åæ ¬∑ f'(z‚ÅΩÀ°‚Åæ)</code>
                                    <code>‚àÇL/‚àÇW‚ÅΩÀ°‚Åæ = ‚àÇL/‚àÇz‚ÅΩÀ°‚Åæ ¬∑ (a‚ÅΩÀ°‚Åª¬π‚Åæ)·µÄ</code>
                                    <code>‚àÇL/‚àÇb‚ÅΩÀ°‚Åæ = ‚àÇL/‚àÇz‚ÅΩÀ°‚Åæ</code>
                                    <code>‚àÇL/‚àÇa‚ÅΩÀ°‚Åª¬π‚Åæ = (W‚ÅΩÀ°‚Åæ)·µÄ ¬∑ ‚àÇL/‚àÇz‚ÅΩÀ°‚Åæ</code>
                                </div>
                            </li>
                            <li>
                                <strong>Update Weights:</strong> Using gradients
                                <div class="formula-box small">
                                    <code>W‚ÅΩÀ°‚Åæ = W‚ÅΩÀ°‚Åæ - Œ± ¬∑ ‚àÇL/‚àÇW‚ÅΩÀ°‚Åæ</code>
                                    <code>b‚ÅΩÀ°‚Åæ = b‚ÅΩÀ°‚Åæ - Œ± ¬∑ ‚àÇL/‚àÇb‚ÅΩÀ°‚Åæ</code>
                                </div>
                            </li>
                        </ol>
                        
                        <div class="info-box">
                            <h4>Chain Rule in Action:</h4>
                            <p>Backpropagation applies the chain rule to propagate errors from output to input:</p>
                            <code>‚àÇL/‚àÇW‚ÅΩ¬π‚Åæ = ‚àÇL/‚àÇa‚ÅΩ·¥∏‚Åæ ¬∑ ‚àÇa‚ÅΩ·¥∏‚Åæ/‚àÇz‚ÅΩ·¥∏‚Åæ ¬∑ ... ¬∑ ‚àÇa‚ÅΩ¬π‚Åæ/‚àÇz‚ÅΩ¬π‚Åæ ¬∑ ‚àÇz‚ÅΩ¬π‚Åæ/‚àÇW‚ÅΩ¬π‚Åæ</code>
                        </div>
                    </div>
                </div>
            </div>

            <!-- Loss Functions -->
            <div class="topic-block">
                <h3 class="topic-title">
                    <span class="topic-number">8</span>
                    Loss Functions
                </h3>
                <div class="topic-content full-width">
                    <p>Loss functions measure how well the network's predictions match the actual values. The goal of training is to minimize this loss.</p>
                    <div class="loss-grid">
                        <div class="loss-card">
                            <h4>Mean Squared Error (MSE)</h4>
                            <div class="formula-box">
                                <code>MSE = (1/n) Œ£(y·µ¢ - ≈∑·µ¢)¬≤</code>
                            </div>
                            <p><strong>Use Case:</strong> Regression problems</p>
                            <p><strong>Characteristics:</strong></p>
                            <ul>
                                <li>Penalizes large errors heavily</li>
                                <li>Always positive</li>
                                <li>Differentiable everywhere</li>
                            </ul>
                        </div>
                        <div class="loss-card">
                            <h4>Binary Cross-Entropy</h4>
                            <div class="formula-box">
                                <code>BCE = -1/n Œ£[y·µ¢log(≈∑·µ¢) + (1-y·µ¢)log(1-≈∑·µ¢)]</code>
                            </div>
                            <p><strong>Use Case:</strong> Binary classification</p>
                            <p><strong>Characteristics:</strong></p>
                            <ul>
                                <li>Measures probability distribution distance</li>
                                <li>Works with sigmoid activation</li>
                                <li>Penalizes confident wrong predictions</li>
                            </ul>
                        </div>
                        <div class="loss-card">
                            <h4>Categorical Cross-Entropy</h4>
                            <div class="formula-box">
                                <code>CCE = -Œ£·µ¢ Œ£‚±º y·µ¢‚±º log(≈∑·µ¢‚±º)</code>
                            </div>
                            <p><strong>Use Case:</strong> Multi-class classification</p>
                            <p><strong>Characteristics:</strong></p>
                            <ul>
                                <li>Works with softmax activation</li>
                                <li>Handles multiple classes</li>
                                <li>One-hot encoded targets</li>
                            </ul>
                        </div>
                        <div class="loss-card">
                            <h4>Mean Absolute Error (MAE)</h4>
                            <div class="formula-box">
                                <code>MAE = (1/n) Œ£|y·µ¢ - ≈∑·µ¢|</code>
                            </div>
                            <p><strong>Use Case:</strong> Regression with outliers</p>
                            <p><strong>Characteristics:</strong></p>
                            <ul>
                                <li>Less sensitive to outliers than MSE</li>
                                <li>Linear penalty for errors</li>
                                <li>More robust</li>
                            </ul>
                        </div>
                    </div>
                </div>
            </div>

            <!-- Optimization Algorithms -->
            <div class="topic-block">
                <h3 class="topic-title">
                    <span class="topic-number">9</span>
                    Optimization Algorithms
                </h3>
                <div class="topic-content full-width">
                    <p>Optimization algorithms determine how the network updates its weights to minimize the loss function.</p>
                    <div class="optimizer-grid">
                        <div class="optimizer-card">
                            <h4>Gradient Descent (GD)</h4>
                            <div class="formula-box">
                                <code>Œ∏ = Œ∏ - Œ± ¬∑ ‚àáJ(Œ∏)</code>
                            </div>
                            <p>The basic optimization algorithm that updates weights using the gradient of the entire dataset.</p>
                            <ul class="pros-cons">
                                <li class="pro">‚úì Stable convergence</li>
                                <li class="pro">‚úì Guaranteed to converge (convex problems)</li>
                                <li class="con">‚úó Slow for large datasets</li>
                                <li class="con">‚úó Requires full dataset per update</li>
                            </ul>
                        </div>
                        <div class="optimizer-card">
                            <h4>Stochastic Gradient Descent (SGD)</h4>
                            <div class="formula-box">
                                <code>Œ∏ = Œ∏ - Œ± ¬∑ ‚àáJ(Œ∏; x·µ¢, y·µ¢)</code>
                            </div>
                            <p>Updates weights using one training example at a time, making it much faster.</p>
                            <ul class="pros-cons">
                                <li class="pro">‚úì Fast updates</li>
                                <li class="pro">‚úì Can escape local minima</li>
                                <li class="pro">‚úì Works with large datasets</li>
                                <li class="con">‚úó Noisy convergence</li>
                                <li class="con">‚úó Requires learning rate tuning</li>
                            </ul>
                        </div>
                        <div class="optimizer-card">
                            <h4>Mini-Batch SGD</h4>
                            <div class="formula-box">
                                <code>Œ∏ = Œ∏ - Œ± ¬∑ ‚àáJ(Œ∏; X·µá·µÉ·µó·∂ú ∞)</code>
                            </div>
                            <p>Balances GD and SGD by using small batches (typically 32-256 examples).</p>
                            <ul class="pros-cons">
                                <li class="pro">‚úì Balance speed and stability</li>
                                <li class="pro">‚úì Efficient computation (GPU)</li>
                                <li class="pro">‚úì Most commonly used</li>
                            </ul>
                        </div>
                        <div class="optimizer-card">
                            <h4>Momentum</h4>
                            <div class="formula-box">
                                <code>v = Œ≤v + Œ±¬∑‚àáJ(Œ∏)</code>
                                <code>Œ∏ = Œ∏ - v</code>
                            </div>
                            <p>Accumulates a velocity vector in directions of persistent gradient reduction.</p>
                            <ul class="pros-cons">
                                <li class="pro">‚úì Faster convergence</li>
                                <li class="pro">‚úì Dampens oscillations</li>
                                <li class="pro">‚úì Escapes plateaus better</li>
                            </ul>
                        </div>
                        <div class="optimizer-card">
                            <h4>Adam (Adaptive Moment)</h4>
                            <div class="formula-box">
                                <code>m = Œ≤‚ÇÅm + (1-Œ≤‚ÇÅ)‚àáJ(Œ∏)</code>
                                <code>v = Œ≤‚ÇÇv + (1-Œ≤‚ÇÇ)(‚àáJ(Œ∏))¬≤</code>
                                <code>Œ∏ = Œ∏ - Œ±¬∑m/‚àö(v+Œµ)</code>
                            </div>
                            <p>Combines momentum and adaptive learning rates. Currently most popular optimizer.</p>
                            <ul class="pros-cons">
                                <li class="pro">‚úì Works well in practice</li>
                                <li class="pro">‚úì Adaptive learning rates</li>
                                <li class="pro">‚úì Requires little tuning</li>
                                <li class="pro">‚úì Good default choice</li>
                            </ul>
                        </div>
                        <div class="optimizer-card">
                            <h4>RMSprop</h4>
                            <div class="formula-box">
                                <code>v = Œ≤v + (1-Œ≤)(‚àáJ(Œ∏))¬≤</code>
                                <code>Œ∏ = Œ∏ - Œ±¬∑‚àáJ(Œ∏)/‚àö(v+Œµ)</code>
                            </div>
                            <p>Adapts learning rate by dividing by exponentially decaying average of squared gradients.</p>
                            <ul class="pros-cons">
                                <li class="pro">‚úì Good for RNNs</li>
                                <li class="pro">‚úì Handles non-stationary objectives</li>
                                <li class="pro">‚úì Works well on noisy data</li>
                            </ul>
                        </div>
                    </div>
                </div>
            </div>

            <!-- Hyperparameters -->
            <div class="topic-block">
                <h3 class="topic-title">
                    <span class="topic-number">10</span>
                    Hyperparameters
                </h3>
                <div class="topic-content full-width">
                    <p>Hyperparameters are configuration settings that control the training process but are not learned from data.</p>
                    <div class="hyperparameter-grid">
                        <div class="hyperparam-card">
                            <div class="param-icon">Œ±</div>
                            <h4>Learning Rate</h4>
                            <p>Controls how much to change weights in response to error.</p>
                            <p><strong>Typical Range:</strong> 0.0001 - 0.1</p>
                            <ul>
                                <li>Too high: Training unstable, diverges</li>
                                <li>Too low: Training very slow</li>
                                <li>Solution: Learning rate scheduling</li>
                            </ul>
                        </div>
                        <div class="hyperparam-card">
                            <div class="param-icon">B</div>
                            <h4>Batch Size</h4>
                            <p>Number of training examples in one forward/backward pass.</p>
                            <p><strong>Typical Range:</strong> 16 - 512</p>
                            <ul>
                                <li>Larger: More stable gradients, faster computation</li>
                                <li>Smaller: More noise, better generalization</li>
                                <li>Common values: 32, 64, 128, 256</li>
                            </ul>
                        </div>
                        <div class="hyperparam-card">
                            <div class="param-icon">E</div>
                            <h4>Epochs</h4>
                            <p>Number of complete passes through the training dataset.</p>
                            <p><strong>Typical Range:</strong> 10 - 1000</p>
                            <ul>
                                <li>Too few: Underfitting</li>
                                <li>Too many: Overfitting</li>
                                <li>Use early stopping to find optimal</li>
                            </ul>
                        </div>
                        <div class="hyperparam-card">
                            <div class="param-icon">H</div>
                            <h4>Hidden Units</h4>
                            <p>Number of neurons in each hidden layer.</p>
                            <p><strong>Typical Range:</strong> 16 - 1024</p>
                            <ul>
                                <li>More units: More capacity to learn</li>
                                <li>Fewer units: Less prone to overfitting</li>
                                <li>Often use powers of 2</li>
                            </ul>
                        </div>
                        <div class="hyperparam-card">
                            <div class="param-icon">L</div>
                            <h4>Number of Layers</h4>
                            <p>Depth of the neural network.</p>
                            <p><strong>Typical Range:</strong> 2 - 100+</p>
                            <ul>
                                <li>More layers: Learn complex hierarchies</li>
                                <li>Deeper networks: Require more data</li>
                                <li>Start shallow, increase if needed</li>
                            </ul>
                        </div>
                        <div class="hyperparam-card">
                            <div class="param-icon">Œª</div>
                            <h4>Regularization</h4>
                            <p>Penalty term to prevent overfitting.</p>
                            <p><strong>Typical Range:</strong> 0.0001 - 0.1</p>
                            <ul>
                                <li>L1: Sparse weights</li>
                                <li>L2: Small weights</li>
                                <li>Dropout: Random neuron deactivation</li>
                            </ul>
                        </div>
                    </div>
                </div>
            </div>

            <!-- Training Techniques -->
            <div class="topic-block">
                <h3 class="topic-title">
                    <span class="topic-number">11</span>
                    Training Techniques
                </h3>
                <div class="technique-grid">
                    <div class="technique-card">
                        <h4>Batch Normalization</h4>
                        <p>Normalizes inputs of each layer to have mean 0 and variance 1.</p>
                        <div class="formula-box small">
                            <code>xÃÇ = (x - Œº) / ‚àö(œÉ¬≤ + Œµ)</code>
                        </div>
                        <p><strong>Benefits:</strong></p>
                        <ul>
                            <li>Faster training</li>
                            <li>Allows higher learning rates</li>
                            <li>Reduces internal covariate shift</li>
                            <li>Acts as regularization</li>
                        </ul>
                    </div>
                    <div class="technique-card">
                        <h4>Dropout</h4>
                        <p>Randomly deactivates neurons during training to prevent overfitting.</p>
                        <p><strong>How it works:</strong> With probability p, set neuron output to 0</p>
                        <p><strong>Benefits:</strong></p>
                        <ul>
                            <li>Prevents co-adaptation of neurons</li>
                            <li>Acts like ensemble learning</li>
                            <li>Simple yet effective</li>
                            <li>Typical p: 0.2 - 0.5</li>
                        </ul>
                    </div>
                    <div class="technique-card">
                        <h4>Early Stopping</h4>
                        <p>Stop training when validation performance stops improving.</p>
                        <p><strong>Process:</strong></p>
                        <ul>
                            <li>Monitor validation loss</li>
                            <li>Keep best model checkpoint</li>
                            <li>Stop if no improvement for n epochs</li>
                            <li>Prevents overfitting</li>
                        </ul>
                    </div>
                    <div class="technique-card">
                        <h4>Data Augmentation</h4>
                        <p>Artificially increase training data by applying transformations.</p>
                        <p><strong>Techniques:</strong></p>
                        <ul>
                            <li>Rotation, flipping, cropping (images)</li>
                            <li>Adding noise</li>
                            <li>Color jittering</li>
                            <li>Improves generalization</li>
                        </ul>
                    </div>
                    <div class="technique-card">
                        <h4>Learning Rate Scheduling</h4>
                        <p>Adjust learning rate during training for better convergence.</p>
                        <p><strong>Strategies:</strong></p>
                        <ul>
                            <li>Step decay: Reduce by factor every n epochs</li>
                            <li>Exponential decay: Multiply by constant < 1</li>
                            <li>Cosine annealing: Follow cosine curve</li>
                            <li>Warm restarts: Periodic resets</li>
                        </ul>
                    </div>
                    <div class="technique-card">
                        <h4>Weight Initialization</h4>
                        <p>Proper initialization prevents vanishing/exploding gradients.</p>
                        <p><strong>Methods:</strong></p>
                        <ul>
                            <li>Xavier/Glorot: For sigmoid/tanh</li>
                            <li>He initialization: For ReLU</li>
                            <li>Random normal with proper variance</li>
                            <li>Critical for deep networks</li>
                        </ul>
                    </div>
                </div>
            </div>
        </div>
    </section>

    <!-- Types of Neural Networks -->
    <section id="types" class="section">
        <div class="container">
            <div class="section-header">
                <span class="section-badge">Architectures</span>
                <h2 class="section-title">Types of Neural Networks</h2>
                <p class="section-description">
                    Different architectures for different problems
                </p>
            </div>

            <div class="types-grid">
                <div class="type-card">
                    <div class="type-header">
                        <div class="type-icon">üîÑ</div>
                        <h3>Feedforward Neural Networks (FNN)</h3>
                    </div>
                    <p class="type-description">The simplest type where information flows in one direction from input to output.</p>
                    <h4>Architecture:</h4>
                    <ul>
                        <li>No cycles or loops</li>
                        <li>Information flows forward only</li>
                        <li>Fully connected layers</li>
                    </ul>
                    <h4>Applications:</h4>
                    <ul>
                        <li>Classification tasks</li>
                        <li>Regression problems</li>
                        <li>Pattern recognition</li>
                    </ul>
                    <div class="type-formula">
                        <strong>Example:</strong> Multi-Layer Perceptron (MLP)
                    </div>
                </div>

                <div class="type-card">
                    <div class="type-header">
                        <div class="type-icon">üñºÔ∏è</div>
                        <h3>Convolutional Neural Networks (CNN)</h3>
                    </div>
                    <p class="type-description">Specialized for processing grid-like data such as images.</p>
                    <h4>Key Components:</h4>
                    <ul>
                        <li><strong>Convolutional Layers:</strong> Extract features using filters/kernels</li>
                        <li><strong>Pooling Layers:</strong> Downsample spatial dimensions</li>
                        <li><strong>Fully Connected Layers:</strong> Final classification</li>
                    </ul>
                    <h4>Important Concepts:</h4>
                    <ul>
                        <li>Local connectivity</li>
                        <li>Parameter sharing</li>
                        <li>Translation invariance</li>
                    </ul>
                    <h4>Applications:</h4>
                    <ul>
                        <li>Image classification (ResNet, VGG, Inception)</li>
                        <li>Object detection (YOLO, R-CNN)</li>
                        <li>Face recognition</li>
                        <li>Medical image analysis</li>
                    </ul>
                    <div class="type-formula">
                        <code>Output = (Input * Kernel) + Bias</code>
                    </div>
                </div>

                <div class="type-card">
                    <div class="type-header">
                        <div class="type-icon">üîÅ</div>
                        <h3>Recurrent Neural Networks (RNN)</h3>
                    </div>
                    <p class="type-description">Designed for sequential data with connections forming cycles.</p>
                    <h4>Architecture:</h4>
                    <ul>
                        <li>Hidden state maintains memory</li>
                        <li>Processes sequences step by step</li>
                        <li>Shares parameters across time</li>
                    </ul>
                    <div class="type-formula">
                        <code>h‚Çú = f(W‚Çïh‚Çú‚Çã‚ÇÅ + W‚Çìx‚Çú + b)</code>
                    </div>
                    <h4>Variants:</h4>
                    <ul>
                        <li><strong>LSTM (Long Short-Term Memory):</strong> Solves vanishing gradient with gates</li>
                        <li><strong>GRU (Gated Recurrent Unit):</strong> Simplified LSTM</li>
                        <li><strong>Bidirectional RNN:</strong> Process sequences in both directions</li>
                    </ul>
                    <h4>Applications:</h4>
                    <ul>
                        <li>Natural Language Processing</li>
                        <li>Speech recognition</li>
                        <li>Time series prediction</li>
                        <li>Machine translation</li>
                    </ul>
                </div>

                <div class="type-card">
                    <div class="type-header">
                        <div class="type-icon">üé≠</div>
                        <h3>Generative Adversarial Networks (GAN)</h3>
                    </div>
                    <p class="type-description">Two networks competing: Generator creates fake data, Discriminator tries to detect it.</p>
                    <h4>Components:</h4>
                    <ul>
                        <li><strong>Generator (G):</strong> Creates synthetic data from random noise</li>
                        <li><strong>Discriminator (D):</strong> Classifies real vs fake data</li>
                    </ul>
                    <h4>Training Process:</h4>
                    <ul>
                        <li>D tries to maximize classification accuracy</li>
                        <li>G tries to fool D (minimize D's accuracy)</li>
                        <li>Min-max game until equilibrium</li>
                    </ul>
                    <div class="type-formula">
                        <code>min_G max_D V(D,G) = E[log(D(x))] + E[log(1-D(G(z)))]</code>
                    </div>
                    <h4>Applications:</h4>
                    <ul>
                        <li>Image generation (StyleGAN)</li>
                        <li>Image-to-image translation (Pix2Pix)</li>
                        <li>Super resolution</li>
                        <li>Deepfakes</li>
                    </ul>
                </div>

                <div class="type-card">
                    <div class="type-header">
                        <div class="type-icon">üîÑ</div>
                        <h3>Autoencoders</h3>
                    </div>
                    <p class="type-description">Unsupervised networks that learn efficient data encodings.</p>
                    <h4>Architecture:</h4>
                    <ul>
                        <li><strong>Encoder:</strong> Compresses input to latent representation</li>
                        <li><strong>Bottleneck:</strong> Low-dimensional latent space</li>
                        <li><strong>Decoder:</strong> Reconstructs input from latent code</li>
                    </ul>
                    <h4>Types:</h4>
                    <ul>
                        <li><strong>Vanilla Autoencoder:</strong> Basic reconstruction</li>
                        <li><strong>Denoising Autoencoder:</strong> Learns to remove noise</li>
                        <li><strong>Variational Autoencoder (VAE):</strong> Probabilistic generative model</li>
                        <li><strong>Sparse Autoencoder:</strong> Enforces sparsity in hidden layer</li>
                    </ul>
                    <h4>Applications:</h4>
                    <ul>
                        <li>Dimensionality reduction</li>
                        <li>Feature learning</li>
                        <li>Anomaly detection</li>
                        <li>Image denoising</li>
                    </ul>
                </div>

                <div class="type-card">
                    <div class="type-header">
                        <div class="type-icon">üéØ</div>
                        <h3>Transformer Networks</h3>
                    </div>
                    <p class="type-description">Modern architecture using self-attention mechanism, revolutionizing NLP.</p>
                    <h4>Key Mechanisms:</h4>
                    <ul>
                        <li><strong>Self-Attention:</strong> Weighs importance of different positions</li>
                        <li><strong>Multi-Head Attention:</strong> Multiple attention mechanisms in parallel</li>
                        <li><strong>Positional Encoding:</strong> Adds sequence order information</li>
                    </ul>
                    <div class="type-formula">
                        <code>Attention(Q,K,V) = softmax(QK·µÄ/‚àöd_k)V</code>
                    </div>
                    <h4>Architecture:</h4>
                    <ul>
                        <li>Encoder-Decoder structure</li>
                        <li>No recurrence needed</li>
                        <li>Parallel processing</li>
                        <li>Layer normalization</li>
                    </ul>
                    <h4>Applications:</h4>
                    <ul>
                        <li>Language models (GPT, BERT)</li>
                        <li>Machine translation</li>
                        <li>Text generation</li>
                        <li>Vision Transformers (ViT)</li>
                    </ul>
                </div>

                <div class="type-card">
                    <div class="type-header">
                        <div class="type-icon">üåê</div>
                        <h3>Radial Basis Function Networks (RBFN)</h3>
                    </div>
                    <p class="type-description">Uses radial basis functions as activation functions.</p>
                    <h4>Architecture:</h4>
                    <ul>
                        <li>Input layer</li>
                        <li>Hidden layer with RBF neurons</li>
                        <li>Linear output layer</li>
                    </ul>
                    <div class="type-formula">
                        <code>œÜ(x) = exp(-||x - c||¬≤ / 2œÉ¬≤)</code>
                    </div>
                    <h4>Applications:</h4>
                    <ul>
                        <li>Function approximation</li>
                        <li>Time series prediction</li>
                        <li>Classification</li>
                    </ul>
                </div>

                <div class="type-card">
                    <div class="type-header">
                        <div class="type-icon">üîÆ</div>
                        <h3>Self-Organizing Maps (SOM)</h3>
                    </div>
                    <p class="type-description">Unsupervised learning for dimensionality reduction and visualization.</p>
                    <h4>Characteristics:</h4>
                    <ul>
                        <li>Competitive learning</li>
                        <li>Topology-preserving mapping</li>
                        <li>2D or 3D visualization of high-dimensional data</li>
                    </ul>
                    <h4>Applications:</h4>
                    <ul>
                        <li>Data visualization</li>
                        <li>Clustering</li>
                        <li>Feature extraction</li>
                    </ul>
                </div>
            </div>
        </div>
    </section>

    <!-- Advanced Topics -->
    <section id="advanced" class="section section-dark">
        <div class="container">
            <div class="section-header">
                <span class="section-badge">Expert Level</span>
                <h2 class="section-title">Advanced Topics</h2>
                <p class="section-description">
                    Deep dive into sophisticated concepts and techniques
                </p>
            </div>

            <div class="advanced-grid">
                <div class="advanced-card">
                    <h3>Transfer Learning</h3>
                    <p>Leverage pre-trained models for new tasks, dramatically reducing training time and data requirements.</p>
                    <h4>Approaches:</h4>
                    <ul>
                        <li><strong>Feature Extraction:</strong> Freeze pre-trained layers, train only final layers</li>
                        <li><strong>Fine-tuning:</strong> Unfreeze some layers and train with small learning rate</li>
                        <li><strong>Domain Adaptation:</strong> Adapt model to new domain</li>
                    </ul>
                    <h4>Popular Pre-trained Models:</h4>
                    <ul>
                        <li>ImageNet models: ResNet, VGG, Inception</li>
                        <li>NLP models: BERT, GPT, T5</li>
                        <li>Multi-modal: CLIP, DALL-E</li>
                    </ul>
                    <p><strong>Benefits:</strong> Requires less data, faster training, better performance</p>
                </div>

                <div class="advanced-card">
                    <h3>Attention Mechanisms</h3>
                    <p>Allow models to focus on relevant parts of input, crucial for sequence-to-sequence tasks.</p>
                    <h4>Types:</h4>
                    <ul>
                        <li><strong>Global Attention:</strong> Attend to all source positions</li>
                        <li><strong>Local Attention:</strong> Focus on subset of positions</li>
                        <li><strong>Self-Attention:</strong> Relate different positions in single sequence</li>
                        <li><strong>Cross-Attention:</strong> Attend from one sequence to another</li>
                    </ul>
                    <div class="formula-box">
                        <code>score(h‚Çú, hÃÑ‚Çõ) = h‚Çú·µÄW‚ÇêhÃÑ‚Çõ</code>
                        <code>Œ±‚Çú‚Çõ = softmax(score(h‚Çú, hÃÑ‚Çõ))</code>
                        <code>c‚Çú = Œ£‚Çõ Œ±‚Çú‚ÇõhÃÑ‚Çõ</code>
                    </div>
                </div>

                <div class="advanced-card">
                    <h3>Regularization Techniques</h3>
                    <p>Methods to prevent overfitting and improve generalization.</p>
                    <h4>L1 Regularization (Lasso):</h4>
                    <div class="formula-box small">
                        <code>Loss = MSE + Œª Œ£|w·µ¢|</code>
                    </div>
                    <p>Promotes sparsity, feature selection</p>
                    
                    <h4>L2 Regularization (Ridge):</h4>
                    <div class="formula-box small">
                        <code>Loss = MSE + Œª Œ£w·µ¢¬≤</code>
                    </div>
                    <p>Prevents large weights, smoother models</p>
                    
                    <h4>Elastic Net:</h4>
                    <div class="formula-box small">
                        <code>Loss = MSE + Œª‚ÇÅŒ£|w·µ¢| + Œª‚ÇÇŒ£w·µ¢¬≤</code>
                    </div>
                    <p>Combines L1 and L2</p>
                </div>

                <div class="advanced-card">
                    <h3>Gradient Problems</h3>
                    <p>Common issues in training deep networks.</p>
                    <h4>Vanishing Gradients:</h4>
                    <ul>
                        <li><strong>Problem:</strong> Gradients become very small in early layers</li>
                        <li><strong>Causes:</strong> Deep networks, sigmoid/tanh activation</li>
                        <li><strong>Solutions:</strong> ReLU, Batch Normalization, ResNet (skip connections)</li>
                    </ul>
                    <h4>Exploding Gradients:</h4>
                    <ul>
                        <li><strong>Problem:</strong> Gradients become very large</li>
                        <li><strong>Symptoms:</strong> NaN values, model divergence</li>
                        <li><strong>Solutions:</strong> Gradient clipping, proper weight initialization, lower learning rate</li>
                    </ul>
                </div>

                <div class="advanced-card">
                    <h3>Residual Networks (ResNet)</h3>
                    <p>Introduces skip connections to enable training of very deep networks.</p>
                    <div class="formula-box">
                        <code>y = F(x, {W·µ¢}) + x</code>
                    </div>
                    <h4>Key Concepts:</h4>
                    <ul>
                        <li><strong>Skip Connections:</strong> Add input directly to output</li>
                        <li><strong>Identity Mapping:</strong> Easier to learn residual F(x)</li>
                        <li><strong>Benefits:</strong> Train networks with 100+ layers</li>
                    </ul>
                    <h4>Advantages:</h4>
                    <ul>
                        <li>Solves vanishing gradient</li>
                        <li>Enables very deep architectures</li>
                        <li>Better gradient flow</li>
                        <li>State-of-the-art performance</li>
                    </ul>
                </div>

                <div class="advanced-card">
                    <h3>Capsule Networks</h3>
                    <p>Novel architecture using capsules instead of neurons to better capture spatial hierarchies.</p>
                    <h4>Capsules:</h4>
                    <ul>
                        <li>Output vectors instead of scalars</li>
                        <li>Vector length = probability of entity existence</li>
                        <li>Vector direction = entity properties</li>
                    </ul>
                    <h4>Dynamic Routing:</h4>
                    <p>Iterative process to determine how capsules communicate</p>
                    <h4>Advantages:</h4>
                    <ul>
                        <li>Better handling of spatial relationships</li>
                        <li>Viewpoint invariance</li>
                        <li>Fewer parameters for better performance</li>
                    </ul>
                </div>

                <div class="advanced-card">
                    <h3>Neural Architecture Search (NAS)</h3>
                    <p>Automated method to discover optimal neural network architectures.</p>
                    <h4>Approaches:</h4>
                    <ul>
                        <li><strong>Reinforcement Learning:</strong> RL agent designs architectures</li>
                        <li><strong>Evolutionary Algorithms:</strong> Evolve architectures over generations</li>
                        <li><strong>Gradient-based:</strong> DARTS, differentiable search</li>
                    </ul>
                    <h4>Search Space:</h4>
                    <ul>
                        <li>Layer types and connections</li>
                        <li>Number of layers</li>
                        <li>Hyperparameters</li>
                    </ul>
                    <p><strong>Challenge:</strong> Computationally expensive</p>
                </div>

                <div class="advanced-card">
                    <h3>Few-Shot Learning</h3>
                    <p>Learning from very few examples, mimicking human learning capability.</p>
                    <h4>Approaches:</h4>
                    <ul>
                        <li><strong>Meta-Learning:</strong> Learning to learn (MAML)</li>
                        <li><strong>Metric Learning:</strong> Learn similarity metrics</li>
                        <li><strong>Memory-Augmented:</strong> External memory mechanisms</li>
                    </ul>
                    <h4>Scenarios:</h4>
                    <ul>
                        <li>One-shot learning: 1 example per class</li>
                        <li>Few-shot: 2-5 examples per class</li>
                        <li>Zero-shot: No examples, only descriptions</li>
                    </ul>
                </div>

                <div class="advanced-card">
                    <h3>Explainable AI (XAI)</h3>
                    <p>Techniques to interpret and explain neural network decisions.</p>
                    <h4>Methods:</h4>
                    <ul>
                        <li><strong>LIME:</strong> Local Interpretable Model-agnostic Explanations</li>
                        <li><strong>SHAP:</strong> SHapley Additive exPlanations</li>
                        <li><strong>Grad-CAM:</strong> Gradient-weighted Class Activation Mapping</li>
                        <li><strong>Attention Visualization:</strong> Show what model attends to</li>
                    </ul>
                    <h4>Importance:</h4>
                    <ul>
                        <li>Trust and transparency</li>
                        <li>Debugging models</li>
                        <li>Regulatory compliance</li>
                        <li>Identifying biases</li>
                    </ul>
                </div>

                <div class="advanced-card">
                    <h3>Adversarial Training</h3>
                    <p>Make models robust against adversarial attacks.</p>
                    <h4>Adversarial Examples:</h4>
                    <p>Small perturbations to input that fool the model</p>
                    <div class="formula-box small">
                        <code>x' = x + Œµ ¬∑ sign(‚àá‚ÇìL(Œ∏, x, y))</code>
                    </div>
                    <h4>Defense Strategies:</h4>
                    <ul>
                        <li>Train on adversarial examples</li>
                        <li>Defensive distillation</li>
                        <li>Input transformations</li>
                        <li>Certified defenses</li>
                    </ul>
                </div>

                <div class="advanced-card">
                    <h3>Neural ODEs</h3>
                    <p>Continuous-depth models using ordinary differential equations.</p>
                    <div class="formula-box">
                        <code>dh(t)/dt = f(h(t), t, Œ∏)</code>
                    </div>
                    <h4>Advantages:</h4>
                    <ul>
                        <li>Memory efficient</li>
                        <li>Continuous transformations</li>
                        <li>Adaptive computation</li>
                        <li>Normalizing flows</li>
                    </ul>
                    <p><strong>Use Cases:</strong> Time series, generative models, sequential data</p>
                </div>

                <div class="advanced-card">
                    <h3>Pruning and Compression</h3>
                    <p>Reduce model size and computational requirements.</p>
                    <h4>Techniques:</h4>
                    <ul>
                        <li><strong>Weight Pruning:</strong> Remove unimportant weights</li>
                        <li><strong>Neuron Pruning:</strong> Remove entire neurons</li>
                        <li><strong>Knowledge Distillation:</strong> Train small model from large model</li>
                        <li><strong>Quantization:</strong> Reduce precision (FP32 ‚Üí INT8)</li>
                    </ul>
                    <h4>Benefits:</h4>
                    <ul>
                        <li>Faster inference</li>
                        <li>Lower memory usage</li>
                        <li>Deploy on edge devices</li>
                        <li>Often minimal accuracy loss</li>
                    </ul>
                </div>
            </div>
        </div>
    </section>

    <!-- Best Practices Section -->
    <section id="best-practices" class="section">
        <div class="container">
            <div class="section-header">
                <span class="section-badge">Guidelines</span>
                <h2 class="section-title">Best Practices & Tips</h2>
            </div>
            <div class="practices-grid">
                <div class="practice-card">
                    <div class="practice-number">1</div>
                    <h4>Data Preparation</h4>
                    <ul>
                        <li>Normalize/standardize inputs</li>
                        <li>Handle missing values</li>
                        <li>Split data properly (train/val/test)</li>
                        <li>Use data augmentation</li>
                        <li>Balance classes if needed</li>
                    </ul>
                </div>
                <div class="practice-card">
                    <div class="practice-number">2</div>
                    <h4>Start Simple</h4>
                    <ul>
                        <li>Begin with simple model</li>
                        <li>Establish baseline</li>
                        <li>Gradually increase complexity</li>
                        <li>Monitor overfitting</li>
                    </ul>
                </div>
                <div class="practice-card">
                    <div class="practice-number">3</div>
                    <h4>Monitor Training</h4>
                    <ul>
                        <li>Plot loss curves</li>
                        <li>Check train/val gap</li>
                        <li>Use TensorBoard/Wandb</li>
                        <li>Save checkpoints</li>
                        <li>Log metrics</li>
                    </ul>
                </div>
                <div class="practice-card">
                    <div class="practice-number">4</div>
                    <h4>Hyperparameter Tuning</h4>
                    <ul>
                        <li>Use grid/random search</li>
                        <li>Try Bayesian optimization</li>
                        <li>Start with default Adam optimizer</li>
                        <li>Tune learning rate first</li>
                    </ul>
                </div>
                <div class="practice-card">
                    <div class="practice-number">5</div>
                    <h4>Debugging</h4>
                    <ul>
                        <li>Overfit single batch first</li>
                        <li>Check gradient flow</li>
                        <li>Visualize activations</li>
                        <li>Use gradient checking</li>
                    </ul>
                </div>
                <div class="practice-card">
                    <div class="practice-number">6</div>
                    <h4>Deployment</h4>
                    <ul>
                        <li>Optimize model size</li>
                        <li>Use appropriate precision</li>
                        <li>Test on edge cases</li>
                        <li>Monitor in production</li>
                    </ul>
                </div>
            </div>
        </div>
    </section>

    <!-- Footer -->
    <footer class="footer">
        <div class="container">
            <div class="footer-content">
                <div class="footer-section">
                    <div class="footer-logo">
                        <span class="logo-icon">üß†</span>
                        <span class="logo-text">NeuraNet</span>
                    </div>
                    <p>Your comprehensive guide to understanding neural networks from basics to advanced concepts.</p>
                </div>
                <div class="footer-section">
                    <h4>Quick Links</h4>
                    <ul class="footer-links">
                        <li><a href="#basics">Basics</a></li>
                        <li><a href="#architecture">Architecture</a></li>
                        <li><a href="#training">Training</a></li>
                        <li><a href="#types">Types</a></li>
                    </ul>
                </div>
                <div class="footer-section">
                    <h4>Resources</h4>
                    <ul class="footer-links">
                        <li><a href="#advanced">Advanced Topics</a></li>
                        <li><a href="#best-practices">Best Practices</a></li>
                    </ul>
                </div>
            </div>
            <div class="footer-bottom">
                <p>&copy; 2025 NeuraNet. Built with passion, Curated by Prateek Dutta.</p>
            </div>
        </div>
    </footer>

    <script src="script.js"></script>
</body>
</html>
